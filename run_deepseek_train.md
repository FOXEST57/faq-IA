Absolument ! Voici un script ZSH complet pour automatiser l'entraÃ®nement de votre modÃ¨le DeepSeek sur macOS avec GPU M2/M3. Ce script gÃ¨re l'installation des dÃ©pendances, la prÃ©paration des donnÃ©es et le lancement de l'entraÃ®nement.

```zsh:run_deepseek_train.zsh
#!/bin/zsh

# ----------------------------------------------------------------------
# DeepSeek Fine-Tuning Script for Apple Silicon (M1/M2/M3)
# OptimisÃ© pour GPU MPS - macOS Sonoma+
# ----------------------------------------------------------------------

# ======= CONFIGURATION ======= (Modifiez ces valeurs selon vos besoins)
DATA_DIR="training_data"        # Dossier contenant vos CSV/JSON
MODEL_NAME="deepseek-ai/deepseek-coder-1.3b-base"  # ModÃ¨le Ã  fine-tuner
EPOCHS=3                        # Nombre d'Ã©poques d'entraÃ®nement
BATCH_SIZE=4                    # Taille de batch (2-8 selon RAM)
# =============================

echo "ğŸš€ DÃ©marrage du processus de fine-tuning DeepSeek sur $(sw_vers -productName) $(sw_vers -productVersion)"

# CrÃ©er l'environnement virtuel
echo "\nğŸ”§ CrÃ©ation de l'environnement Python..."
python3 -m venv deepseek-env
source deepseek-env/bin/activate

# Installer les dÃ©pendances avec optimisation MPS
echo "\nğŸ“¦ Installation des dÃ©pendances (peut prendre 5-10 min)..."
pip install --upgrade pip > /dev/null
pip install torch transformers datasets accelerate peft bitsandbytes pandas pyarrow > /dev/null

# VÃ©rifier la compatibilitÃ© MPS
echo "\nğŸ” VÃ©rification de la compatibilitÃ© GPU:"
python -c "import torch; print(f'â–º MPS disponible: {torch.backends.mps.is_available()}\nâ–º MPS activÃ©: {torch.backends.mps.is_built()}')"

# GÃ©nÃ©rer le script d'entraÃ®nement
echo "\nğŸ› ï¸ GÃ©nÃ©ration du script d'entraÃ®nement..."
cat > deepseek_train.py <<EOL
import os
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model
import torch

# Configuration
model_id = "$MODEL_NAME"
data_path = "$DATA_DIR"
output_dir = "./deepseek_finetuned"
os.makedirs(output_dir, exist_ok=True)

# 1. Chargement des donnÃ©es
print("\\nğŸ“‚ Chargement des donnÃ©es...")
dfs = []
for file in os.listdir(data_path):
    if file.endswith('.csv'):
        dfs.append(pd.read_csv(f"{data_path}/{file}"))
    elif file.endswith('.json'):
        dfs.append(pd.read_json(f"{data_path}/{file}"))

if not dfs:
    raise ValueError("Aucune donnÃ©e trouvÃ©e! VÃ©rifiez le dossier $DATA_DIR")

dataset = Dataset.from_pandas(pd.concat(dfs))

# 2. PrÃ©paration du tokenizer
print("ğŸ”¡ Initialisation du tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(
        examples["text"],  # Colonne contenant le texte
        padding="max_length",
        truncation=True,
        max_length=512
    )

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# 3. Configuration 4-bit QLoRA
print("âš™ï¸ Configuration du modÃ¨le (4-bit QLoRA)...")
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    use_mps_device=True
)

# 4. ParamÃ¨tres LoRA
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# 5. EntraÃ®nement
print("ğŸš‚ Lancement de l'entraÃ®nement (peut prendre du temps)...")
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=$EPOCHS,
    per_device_train_batch_size=$BATCH_SIZE,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    optim="paged_adamw_8bit",
    fp16=True,
    logging_steps=10,
    use_mps_device=True,
    report_to="none",
    save_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

trainer.train()

# 6. Sauvegarde finale
print("ğŸ’¾ Sauvegarde du modÃ¨le fine-tunÃ©...")
model.save_pretrained(f"{output_dir}/final_model")
print(f"âœ… EntraÃ®nement terminÃ©! ModÃ¨le sauvegardÃ© dans: {output_dir}/final_model")
EOL

# Lancer l'entraÃ®nement
echo "\nğŸ§  DÃ©but de l'entraÃ®nement du modÃ¨le (GPU MPS activÃ©)..."
echo "â–º ModÃ¨le: $MODEL_NAME"
echo "â–º Epochs: $EPOCHS"
echo "â–º Taille de batch: $BATCH_SIZE"
echo "â–º DonnÃ©es: $DATA_DIR\n"

python deepseek_train.py

echo "\nâœ¨ OpÃ©ration terminÃ©e! Utilisez 'source deepseek-env/bin/activate' pour activer l'environnement"
```

### Comment utiliser ce script :

1. **Enregistrez le script** dans un fichier `run_deepseek_train.zsh`
2. **Rendez-le exÃ©cutable** :
   ```zsh
   chmod +x run_deepseek_train.zsh
   ```
3. **PrÃ©parez vos donnÃ©es** :
   - CrÃ©ez un dossier `training_data`
   - Placez vos fichiers CSV/JSON dedans
   - Assurez-vous qu'ils contiennent une colonne "text"
4. **ExÃ©cutez le script** :
   ```zsh
   ./run_deepseek_train.zsh
   ```

### FonctionnalitÃ©s clÃ©s :

1. âœ… Installation automatique des dÃ©pendances
2. âœ… Optimisation GPU MPS pour Apple Silicon
3. âœ… Support des formats CSV et JSON
4. âœ… Quantification 4-bit (QLoRA) pour Ã©conomie mÃ©moire
5. âœ… Sauvegarde automatique du modÃ¨le final
6. âœ… Rapport de progression en temps rÃ©el

### Configuration modifiable :

```zsh
# Dans la section CONFIGURATION :
DATA_DIR="votre_dossier_data"   # Dossier contenant les donnÃ©es
MODEL_NAME="deepseek-ai/deepseek-llm-7b-base"  # ModÃ¨le plus grand
EPOCHS=5                        # Plus d'Ã©poches
BATCH_SIZE=2                    # Batch plus petit si mÃ©moire limitÃ©e
```

Pour un modÃ¨le plus performant mais plus gourmand, remplacez par :
```zsh
MODEL_NAME="deepseek-ai/deepseek-llm-7b-base"
```

Ce script est optimisÃ© pour fonctionner sur MacBook Pro M2/M3 avec 16Go+ de RAM. Le processus peut prendre plusieurs heures selon la quantitÃ© de donnÃ©es et la puissance de votre machine.
